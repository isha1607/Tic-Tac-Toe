# -*- coding: utf-8 -*-
"""CC3601.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19HurE4x0q-lCWzU7C7tb01HoO2Xj01NT
"""

import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# Sample DataFrame
data = pd.DataFrame({
    'cv_text': ["Sample CV text 1", "Sample CV text 2", "Sample CV text 3"],
    'extroversion': [1, 0, 1],
    'conscientiousness': [0, 1, 0],
    'openness': [1, 1, 0],
    'agreeableness': [0, 1, 1],
    'neuroticism': [1, 0, 0]
})

# Preprocess text
def preprocess_text(text):
    text = text.lower()
    text = nltk.word_tokenize(text)
    text = [word for word in text if word.isalnum()]
    text = [word for word in text if word not in stop_words]
    return " ".join(text)

data['cv_text'] = data['cv_text'].apply(preprocess_text)

# Tokenize text
tokenizer = Tokenizer(num_words=5000, oov_token="<OOV>")
tokenizer.fit_on_texts(data['cv_text'])
sequences = tokenizer.texts_to_sequences(data['cv_text'])
padded_sequences = pad_sequences(sequences, maxlen=100, padding='post', truncating='post')

# Split data
X_train, X_test, y_train, y_test = train_test_split(padded_sequences, data[['extroversion', 'conscientiousness', 'openness', 'agreeableness', 'neuroticism']], test_size=0.2, random_state=42)

from transformers import BertTokenizer, TFBertModel

bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = TFBertModel.from_pretrained('bert-base-uncased')

def bert_encode(texts, tokenizer, max_len=512):
    all_tokens = []
    all_masks = []
    all_segments = []
    for text in texts:
        tokenized = tokenizer.encode_plus(
            text,
            max_length=max_len,
            padding='max_length',
            truncation=True,
            return_attention_mask=True
        )
        all_tokens.append(tokenized['input_ids'])
        all_masks.append(tokenized['attention_mask'])
        all_segments.append(tokenized['token_type_ids'])
    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)

train_input_ids, train_attention_masks, train_token_type_ids = bert_encode(data['cv_text'].values, bert_tokenizer)

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout

input_ids = Input(shape=(512,), dtype=tf.int32, name="input_ids")
attention_masks = Input(shape=(512,), dtype=tf.int32, name="attention_masks")

bert_output = bert_model(input_ids, attention_mask=attention_masks)[0]
x = tf.keras.layers.GlobalAveragePooling1D()(bert_output)
x = Dropout(0.3)(x)
output = Dense(5, activation='sigmoid')(x)

model = Model(inputs=[input_ids, attention_masks], outputs=output)
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5), loss='binary_crossentropy', metrics=['accuracy'])

history = model.fit([train_input_ids, train_attention_masks], y_train.values, epochs=3, batch_size=16, validation_split=0.1)

def predict_personality(cv_text):
    preprocessed_text = preprocess_text(cv_text)
    encoded_input = bert_encode([preprocessed_text], bert_tokenizer)
    prediction = model.predict(encoded_input)
    traits = ['Extroversion', 'Conscientiousness', 'Openness', 'Agreeableness', 'Neuroticism']
    return {trait: pred for trait, pred in zip(traits, prediction[0])}

# Test the system
cv_text = "Sample CV text to predict personality traits"
predicted_traits = predict_personality(cv_text)
print(predicted_traits)

